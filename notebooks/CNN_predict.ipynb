{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify NetCDF files used for training and prediciton inputs\n",
    "These are low resolution versions of NCAR CAM inputs/outputs, located in the `example_data` directory of this project's git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/adamsjam/git/model_learn\"\n",
    "\n",
    "# files used as feature inputs for model training\n",
    "netcdf_features_train = [root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h0.2001-01-11-00000_lowres.nc\",\n",
    "                         root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h0.2001-01-26-00000_lowres.nc\",\n",
    "                         root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h0.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label inputs for model training\n",
    "netcdf_labels_train = [root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h1.2001-01-11-00000_lowres.nc\",\n",
    "                       root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h1.2001-01-26-00000_lowres.nc\",\n",
    "                       root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h1.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as feature inputs for model prediction\n",
    "netcdf_features_predict = [root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h0.2001-02-25-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label outputs for model prediction\n",
    "netcdf_labels_predict_dense = [root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres_predicted_dense.nc\"]\n",
    "netcdf_labels_predict_conv3d = [root_dir + \"/example_data/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres_predicted_conv3d.nc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets for training and prediction\n",
    "\n",
    "We'll define a function to extract an array of variable(s) for a single level from an xarray DataSet, and another to extract both features and labels from NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def extract_data_array(dataset,\n",
    "                       variables,\n",
    "                       lev):\n",
    "\n",
    "    # allocate the array\n",
    "    arr = np.empty(shape=[dataset.time.size, \n",
    "                          dataset.lat.size, \n",
    "                          dataset.lon.size, \n",
    "                          len(variables)],\n",
    "                   dtype=np.float64)\n",
    "    \n",
    "    # for each variable we'll extract the values \n",
    "    for var_index, var in enumerate(variables):\n",
    "\n",
    "        # if we have (time, lev, lat, lon), then use level parameter\n",
    "        dimensions = dataset.variables[var].dims\n",
    "        if dimensions == ('time', 'lev', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, lev, :, :]\n",
    "        elif dimensions == ('time', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, :, :]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported variable dimensions: {dims}\".format(dims=dimensions))\n",
    "\n",
    "        # add the values into the array at the variable's position\n",
    "        arr[:, :, :, var_index] = values\n",
    "    \n",
    "    return arr\n",
    "    \n",
    "    \n",
    "def extract_features_labels(netdcf_features, \n",
    "                            netcdf_labels,\n",
    "                            feature_vars,\n",
    "                            label_vars,\n",
    "                            level=0):\n",
    "    \"\"\"\n",
    "    Extracts feature and label data from specified NetCDF files for a single level as numpy arrays.\n",
    "    \n",
    "    The feature and label NetCDFs are expected to have matching time, level, lat, and lon coordinate variables.\n",
    "    \n",
    "    Returns two arrays: the first for features and the second for labels. Arrays will have shape (time, lat, lon, var),\n",
    "    where var is the number of feature or label variables. For example if the dimensions of feature data variables in \n",
    "    the NetCDF is (time: 360, lev: 26, lat: 120, lon: 180) and the features specified are [\"T\", \"U\"] then the resulting\n",
    "    features array will have shape (360, 120, 180, 2), with the first feature variable \"T\" corresponding to array[:, :, :, 0]\n",
    "    and the second feature variable \"U\" corresponding to array[:, :, :, 1].\n",
    "    \n",
    "    :param netdcf_features: one or more NetCDF files containing feature variables, can be single file or list\n",
    "    :param netdcf_features: one or more NetCDF files containing label variables, can be single file or list\n",
    "    :param feature_vars: list of feature variable names to be extracted from the features NetCDF\n",
    "    :param label_vars: list of label variable names to be extracted from the labels NetCDF\n",
    "    :param level: index of the level to be extracted (all times/lats/lons at this level for each feature/label variable)\n",
    "    :return: two 4-D numpy arrays, the first for features and the second for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the features (flows) and labels (tendencies) as xarray DataSets\n",
    "    ds_features = xr.open_mfdataset(paths=netdcf_features)\n",
    "    ds_labels = xr.open_mfdataset(paths=netcdf_labels)\n",
    "\n",
    "    # confirm that we have datasets that match on the time, lev, lat, and lon dimension/coordinate\n",
    "    if np.any(ds_features.variables['time'].values != ds_labels.variables['time'].values):\n",
    "        raise ValueError('Non-matching time values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lev'].values != ds_labels.variables['lev'].values):\n",
    "        raise ValueError('Non-matching level values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lat'].values != ds_labels.variables['lat'].values):\n",
    "        raise ValueError('Non-matching lat values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lon'].values != ds_labels.variables['lon'].values):\n",
    "        raise ValueError('Non-matching lon values between feature and label datasets')\n",
    "\n",
    "    # extract feature and label arrays at the specified level\n",
    "    array_features = extract_data_array(ds_features, feature_vars, level)\n",
    "    array_labels = extract_data_array(ds_labels, label_vars, level)\n",
    "    \n",
    "    return array_features, array_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature variables being used are 'PS', 'T', 'U', and 'V'. The label variable is 'PTTEND'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PS\", \"T\", \"U\", \"V\"]\n",
    "labels = [\"PTTEND\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NetCDF files to load feature and label datasets that will be used for training and prediction.\n",
    "\n",
    "All files should share the same time, level, lat, and lon coordinate, with each file's feature or label variables having shape (times, levels, lats, lons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = extract_features_labels(netcdf_features_train[0],\n",
    "                                           netcdf_labels_train[0],\n",
    "                                           features,\n",
    "                                           labels,\n",
    "                                           level=0)\n",
    "predict_x = extract_data_array(xr.open_dataset(netcdf_features_predict[0]),\n",
    "                               features,\n",
    "                               lev=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimension sizes for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_times_train = train_x.shape[0]\n",
    "size_times_predict = train_x.shape[0]\n",
    "size_lat = train_x.shape[1]\n",
    "size_lon = train_x.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our neural network models will work much better if all values are scaled into a range such as between 0 and 1. We'll use scikit-learn's MinMaxScaler for now. The scaler being used for labels will be reused later for inverse scaling of the predicted label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize a list to store scalers for each feature/label\n",
    "scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "\n",
    "# data is 4-D with shape (times, lats, lons, vars), scalers can only work on 2-D arrays,\n",
    "# so for each feature we scale the corresponding 3-D array of values after flattening it,\n",
    "# then reshape back into the original shape\n",
    "for feature_ix in range(len(features)):\n",
    "    scaler = scalers_x[feature_ix]\n",
    "    feature_train = train_x[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "    feature_predict = predict_x[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "    scaled_train = scaler.fit_transform(feature_train)\n",
    "    scaled_predict = scaler.fit_transform(feature_predict)\n",
    "    reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "    reshaped_scaled_predict = np.reshape(scaled_predict, newshape=(size_times_predict, size_lat, size_lon))\n",
    "    train_x[:, :, :, feature_ix] = reshaped_scaled_train\n",
    "    predict_x[:, :, :, feature_ix] = reshaped_scaled_predict\n",
    "for label_ix in range(len(labels)):\n",
    "    scaler = scalers_y[label_ix]\n",
    "    label_train = train_y[:, :, :, label_ix].flatten().reshape(-1, 1)\n",
    "    scaled_train = scaler.fit_transform(label_train)\n",
    "    reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "    train_y[:, :, :, label_ix] = reshaped_scaled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras models to use for prediciton\n",
    "We'll define two neural network models using the Keras library with TensorFlow as its backend. One of these models will contain only simple densely connected layers, and another will contain a both convolutional layer and a densely connected layer. We'll use both of these for prediction of labels corresponding to the results of NCAR CAM model runs involving computation of the Held-Suarez test case. Initially we'll focus on the input feature variables PS, T, U, and V and the output label PTTEND.\n",
    "\n",
    "##### Dense layer-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 69\n",
      "Trainable params: 69\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "dense_model = Sequential()\n",
    "\n",
    "# add a fully-connected hidden layer with the same number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features), input_dim=len(features), activation='relu'))\n",
    "\n",
    "# add a fully-connected hidden layer with the twice the number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "dense_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "dense_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 720, 12, 23, 32)   3488      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 720, 12, 23, 8)    264       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 720, 12, 23, 1)    9         \n",
      "=================================================================\n",
      "Total params: 3,761\n",
      "Trainable params: 3,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Dense\n",
    "\n",
    "# define the model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# add an initial 3-D convolutional layer\n",
    "cnn_model.add(Conv3D(filters=32,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation=\"relu\",\n",
    "                     data_format=\"channels_last\",\n",
    "                     input_shape=(size_times_train, size_lat, size_lon, len(features)),\n",
    "                     padding='same'))\n",
    "\n",
    "# add a fully-connected hidden layer with twice the number of neurons as input attributes (features)\n",
    "cnn_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "cnn_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape data for model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = (1, ) + train_x.shape\n",
    "shape_y = (1, ) + train_y.shape\n",
    "train_x = np.reshape(train_x, newshape=shape_x)\n",
    "train_y = np.reshape(train_y, newshape=shape_y)\n",
    "predict_x = np.reshape(predict_x, newshape=shape_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the models for the first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 2s - loss: 0.3783\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.2326\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.1847\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.1725\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.1593\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.1469\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.1341\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.1208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b873ae901d0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_x, train_y, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
