{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NetCDF files to load feature and label datasets that will be used for training the model.\n",
    "\n",
    "All files will share the same time, level, lat, and lon dimensions, ie. each file's feature or label variable having shape (times, levels, lats, lons).\n",
    "\n",
    "We'll use three input files for training that contain 720 time steps (for a total of 2160 training time steps), and then use the trained model to predict label values corresponding to a single features input dataset which contains 720 time steps.  \n",
    "\n",
    "The feature variables being used are 'PS', 'T', 'U', and 'V'. The label variable is 'PTTEND'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "netcdf_features_train = [\"/home/adamsjam/data/lowres/fv091x180L26_moist_HS.cam.h0.2001-01-11-00000_lowres.nc\",\n",
    "                         \"/home/adamsjam/data/lowres/fv091x180L26_moist_HS.cam.h0.2001-01-26-00000_lowres.nc\"]\n",
    "netcdf_labels_train = [\"/home/adamsjam/data/lowres/fv091x180L26_moist_HS.cam.h1.2001-01-11-00000_lowres.nc\",\n",
    "                       \"/home/adamsjam/data/lowres/fv091x180L26_moist_HS.cam.h1.2001-01-26-00000_lowres.nc\"]\n",
    "netcdf_features_predict = [\"/home/adamsjam/data/lowres/fv091x180L26_moist_HS.cam.h0.2001-02-10-00000_lowres.nc\"]\n",
    "                         \n",
    "# # open the features (flows) and labels (tendencies) as xarray DataSets\n",
    "# ds_train_features = xr.open_mfdataset(paths=netcdf_features_train)\n",
    "# ds_train_labels = xr.open_mfdataset(paths=netcdf_labels_train)\n",
    "# ds_predict_features = xr.open_mfdataset(paths=netcdf_features_predict)\n",
    "\n",
    "# # confirm that we have training datasets that match on the time, lev, lat, and lon dimension/coordinate\n",
    "# if np.any(ds_train_features.variables['time'].values != ds_train_labels.variables['time'].values):\n",
    "#     raise ValueError('Non-matching time values between feature and label datasets')\n",
    "# if np.any(ds_train_features.variables['lev'].values != ds_train_labels.variables['lev'].values):\n",
    "#     raise ValueError('Non-matching level values between feature and label datasets')\n",
    "# if np.any(ds_train_features.variables['lat'].values != ds_train_labels.variables['lat'].values):\n",
    "#     raise ValueError('Non-matching lat values between feature and label datasets')\n",
    "# if np.any(ds_train_features.variables['lon'].values != ds_train_labels.variables['lon'].values):\n",
    "#     raise ValueError('Non-matching lon values between feature and label datasets')\n",
    "\n",
    "# # confirm that the training and prediction datasets match on the lev, lat, and lon dimension/coordinate\n",
    "# # it's likely that we'll use more times for training than for prediction, so we ignore those differences\n",
    "# if np.any(ds_train_features.variables['lev'].values != ds_predict_features.variables['lev'].values):\n",
    "#     raise ValueError('Non-matching level values between train and predict feature datasets')\n",
    "# if np.any(ds_train_features.variables['lat'].values != ds_predict_features.variables['lat'].values):\n",
    "#     raise ValueError('Non-matching lat values between train and predict feature datasets')\n",
    "# if np.any(ds_train_features.variables['lon'].values != ds_predict_features.variables['lon'].values):\n",
    "#     raise ValueError('Non-matching lon values between train and predict feature datasets')\n",
    "\n",
    "# train/fit/score models using the dry features and corresponding labels\n",
    "features = ['PS', 'T', 'U', 'V']\n",
    "labels = ['PTTEND']\n",
    "        \n",
    "# # trim out all non-relevant data variables from the datasets\n",
    "# for var in ds_train_features.data_vars:\n",
    "#     if var not in features:\n",
    "#         ds_train_features = ds_train_features.drop(var)\n",
    "# for var in ds_train_labels.data_vars:\n",
    "#     if var not in labels:\n",
    "#         ds_train_labels = ds_train_labels.drop(var)\n",
    "# for var in ds_predict_features.data_vars:\n",
    "#     if var not in features:\n",
    "#         ds_predict_features = ds_predict_features.drop(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load single training dataset\n",
    "\n",
    "We'll define a function to read arrays of features and labels from NetCDF for a single level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_label_data(netdcf_features, \n",
    "                            netcdf_labels,\n",
    "                            feature_vars,\n",
    "                            label_vars,\n",
    "                            level=0):\n",
    "    \"\"\"\n",
    "    Loads feature and label data from specified NetCDF files for a single level.\n",
    "    \n",
    "    The feature and label NetCDFs are expected to have matching time, level, lat, and lon coordinate variables.\n",
    "    \n",
    "    Returns two arrays: the first for features and the second for labels. Arrays will have shape (time, lat, lon, var),\n",
    "    where var is the number of feature or label variables. For example if the dimensions of feature data variables in \n",
    "    the NetCDF is (time: 360, lev: 26, lat: 120, lon: 180) and the features specified are [\"T\", \"U\"] then the resulting\n",
    "    features array will have shape (360, 120, 180, 2), with the first feature variable \"T\" corresponding to array[:, :, :, 0]\n",
    "    and the second feature variable \"U\" corresponding to array[:, :, :, 1].\n",
    "    \n",
    "    :param netdcf_features: one or more NetCDF files containing feature variables, can be single file or list\n",
    "    :param netdcf_features: one or more NetCDF files containing label variables, can be single file or list\n",
    "    :param feature_vars: list of feature variable names to be extracted from the features NetCDF\n",
    "    :param label_vars: list of label variable names to be extracted from the labels NetCDF\n",
    "    :param level: index of the level to be extracted (all times/lats/lons at this level for each feature/label variable)\n",
    "    :return: two 4-D numpy arrays, the first for features and the second for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the features (flows) and labels (tendencies) as xarray DataSets\n",
    "    ds_features = xr.open_mfdataset(paths=netdcf_features)\n",
    "    ds_labels = xr.open_mfdataset(paths=netcdf_labels)\n",
    "\n",
    "    # confirm that we have datasets that match on the time, lev, lat, and lon dimension/coordinate\n",
    "    if np.any(ds_features.variables['time'].values != ds_labels.variables['time'].values):\n",
    "        raise ValueError('Non-matching time values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lev'].values != ds_labels.variables['lev'].values):\n",
    "        raise ValueError('Non-matching level values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lat'].values != ds_labels.variables['lat'].values):\n",
    "        raise ValueError('Non-matching lat values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lon'].values != ds_labels.variables['lon'].values):\n",
    "        raise ValueError('Non-matching lon values between feature and label datasets')\n",
    "\n",
    "    # allocate arrays\n",
    "    array_features = np.empty(shape=[ds_features.time.size, \n",
    "                                     ds_features.lat.size, \n",
    "                                     ds_features.lon.size, \n",
    "                                     len(feature_vars)], \n",
    "                              dtype=np.float64)\n",
    "    array_labels = np.empty(shape=[ds_features.time.size, \n",
    "                                   ds_features.lat.size, \n",
    "                                   ds_features.lon.size, \n",
    "                                   len(label_vars)], \n",
    "                            dtype=np.float64)\n",
    "    \n",
    "    # get variable vales into the arrays for both features and labels\n",
    "    for vars, dataset, arr in zip([feature_vars, label_vars], \n",
    "                                  [ds_features, ds_labels], \n",
    "                                  [array_features, array_labels]):\n",
    "        \n",
    "        # for each variable we'll extract the values \n",
    "        for var_index, var in enumerate(vars):\n",
    "\n",
    "            # if we have (time, lev, lat, lon), then use level parameter\n",
    "            dimensions = dataset.variables[var].dims\n",
    "            if dimensions == ('time', 'lev', 'lat', 'lon'):\n",
    "                values = dataset[var].values[:, level, :, :]\n",
    "            elif dimensions == ('time', 'lat', 'lon'):\n",
    "                values = dataset[var].values[:, :, :]\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported feature variable dimensions: {dims}\".format(dims=dimensions))\n",
    "\n",
    "        # add the values into the array at the variable's position\n",
    "        arr[:, :, :, var_index] = values\n",
    "\n",
    "    return array_features, array_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = load_feature_label_data(netcdf_features_train[0],\n",
    "                                           netcdf_labels_train[0],\n",
    "                                           features,\n",
    "                                           labels,\n",
    "                                           level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 12, 23, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a single file's time steps as a measure of how many time steps we should use as a unit of data. We'll use three of these as inputs to the model for training, then a single unit of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size_times_train = ds_train_features.variables['time'].size\n",
    "# size_times_predict = ds_predict_features.variables['time'].size\n",
    "# size_lat = ds_predict_features.variables['lat'].size\n",
    "# size_lon = ds_predict_features.variables['lon'].size\n",
    "\n",
    "size_times_train = train_x.shape[0]\n",
    "# size_times_predict = ds_predict_features.variables['time'].size\n",
    "size_lat = train_x.shape[1]\n",
    "size_lon = train_x.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to pull an array of data for a single level, i.e. all times, lats, lons, and which returns an array with shape: `(times, lats, lons, variables)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_vars_into_array(dataset,\n",
    "                         variables,\n",
    "                         level):\n",
    "    \"\"\"\n",
    "    Create a Numpy array from the specified variables of an xarray DataSet.\n",
    "\n",
    "    :param dataset: xarray.DataSet\n",
    "    :param variables: list of variables to be extracted from the DataSet and included in the resulting DataFrame\n",
    "    :param level: the level index (all times, lats, and lons included from this indexed level)\n",
    "    :return: an array with shape (ds.time.size, ds.lat.size, ds.lon.size, len(variables)) and dtype float\n",
    "    \"\"\"\n",
    "\n",
    "    # the array we'll populate and return\n",
    "    arr = np.empty(shape=[dataset.time.size, dataset.lat.size, dataset.lon.size, len(variables)], dtype=float)\n",
    "\n",
    "    # loop over each variable, adding each into the array\n",
    "    for index, var in enumerate(variables):\n",
    "\n",
    "        # if we have (time, lev, lat, lon), then use level parameter\n",
    "        dimensions = dataset.variables[var].dims\n",
    "        if dimensions == ('time', 'lev', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, level, :, :]\n",
    "        elif dimensions == ('time', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, :, :]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported variable dimensions: {dims}\".format(dims=dimensions))\n",
    "\n",
    "        # add the values into the array at the variable's position\n",
    "        arr[:, :, :, index] = values\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an array of training features and labels, and a corresponding array of features to use for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # use the first level\n",
    "# lev = 0\n",
    "\n",
    "# # get the array of training features for this level (all times/lats/lons)\n",
    "# train_x = pull_vars_into_array(ds_train_features,\n",
    "#                                features,\n",
    "#                                lev)\n",
    "\n",
    "# # get the array of training labels for this level (all times/lats/lons)\n",
    "# train_y = pull_vars_into_array(ds_train_labels,\n",
    "#                                labels,\n",
    "#                                lev)\n",
    "\n",
    "# # get the array of features from which we'll predict new label(s)\n",
    "# predict_x = pull_vars_into_array(ds_predict_features,\n",
    "#                                  features,\n",
    "#                                  lev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model will work much better if all values are scaled into a range such as between 0 and 1. We'll use scikit-learn's MinMaxScaler for now. The scaler being used for labels will be reused later for inverse scaling the predicted label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize a list to store scalers for each feature/label\n",
    "scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "\n",
    "# data is 4-D with shape (times, lats, lons, vars), scalers can only work on 2-D arrays,\n",
    "# so for each feature we scale the corresponding 3-D array of values after flattening it,\n",
    "# then reshape back into the original shape\n",
    "for feature_ix in range(len(features)):\n",
    "    scaler = scalers_x[feature_ix]\n",
    "    feature_train = train_x[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "#     feature_predict = predict_x[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "    scaled_train = scaler.fit_transform(feature_train)\n",
    "#     scaled_predict = scaler.fit_transform(feature_predict)\n",
    "    reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "#     reshaped_scaled_predict = np.reshape(scaled_predict, newshape=(size_times_predict, size_lat, size_lon))\n",
    "    train_x[:, :, :, feature_ix] = reshaped_scaled_train\n",
    "#     predict_x[:, :, :, feature_ix] = reshaped_scaled_predict\n",
    "for label_ix in range(len(labels)):\n",
    "    scaler = scalers_y[label_ix]\n",
    "    label_train = train_y[:, :, :, label_ix].flatten().reshape(-1, 1)\n",
    "    scaled_train = scaler.fit_transform(label_train)\n",
    "    reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "    train_y[:, :, :, label_ix] = reshaped_scaled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A batch of data for the model will be the full lat/lon grid for a single level geospatially, and the number of time steps of the individual files, since we'll predict for a single file's worth of data. For example we're using three files worth of data for training the model, then using the fitted model to predict the label variables corresponding to a single file of feature inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batches = int(size_times_train / size_times_predict)\n",
    "times_per_batch = int(size_times_train / training_batches)\n",
    "training_features_shape = (training_batches, times_per_batch, size_lat, size_lon, len(features))\n",
    "training_labels_shape = (training_batches, times_per_batch, size_lat, size_lon, len(labels))\n",
    "predict_features_shape = (1, times_per_batch, size_lat, size_lon, len(features))\n",
    "train_x = np.reshape(train_x, newshape=training_features_shape)\n",
    "train_y = np.reshape(train_y, newshape=training_labels_shape)\n",
    "predict_x = np.reshape(predict_x, newshape=predict_features_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model using convolutional and dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 720, 12, 23, 32)   3488      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 720, 12, 23, 4)    132       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 720, 12, 23, 1)    5         \n",
      "=================================================================\n",
      "Total params: 3,625\n",
      "Trainable params: 3,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Dense\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "\n",
    "# add an initial 3-D convolutional layer\n",
    "model.add(Conv3D(filters=32,\n",
    "                 kernel_size=(3, 3, 3),\n",
    "                 activation=\"relu\",\n",
    "                 data_format=\"channels_last\",\n",
    "                 input_shape=(size_times_train, size_lat, size_lon, len(features)),\n",
    "                 padding='same'))\n",
    "\n",
    "# add a fully-connected hidden layer with the number of neurons as input attributes (features)\n",
    "model.add(Dense(len(features), activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display summary info\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model for the first level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv3d_1_input to have 5 dimensions, but got array with shape (720, 12, 23, 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e8fc8662ba03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#model.fit(train_x, train_y, batch_size=times_per_batch, shuffle=True, epochs=8, verbose=2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv3d_1_input to have 5 dimensions, but got array with shape (720, 12, 23, 4)"
     ]
    }
   ],
   "source": [
    "#model.fit(train_x, train_y, batch_size=times_per_batch, shuffle=True, epochs=8, verbose=2)\n",
    "model.fit(train_x, train_y, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
