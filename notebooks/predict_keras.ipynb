{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate modeling with Keras\n",
    "This notebook illustrates the process of creating simple neural network models using the Keras framework, processing data for input to the models for training and prediction, and the use of numpy and xarray for data wrangling and I/O with datasets contained within NetCDF files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify NetCDF files used for training and prediciton inputs\n",
    "These are low resolution versions of NCAR CAM inputs/outputs, located in the `example_data` directory of this project's git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/adamsjam/git/model_learn/example_data\"\n",
    "result_dir = \"/home/adamsjam/test\"\n",
    "\n",
    "# files used as feature inputs for model training\n",
    "netcdf_features_train = [data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-01-11-00000_lowres.nc\",\n",
    "                         data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-01-26-00000_lowres.nc\",\n",
    "                         data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label inputs for model training\n",
    "netcdf_labels_train = [data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-01-11-00000_lowres.nc\",\n",
    "                       data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-01-26-00000_lowres.nc\",\n",
    "                       data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as feature inputs for model prediction\n",
    "netcdf_features_predict = [data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-02-25-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label outputs for model prediction\n",
    "netcdf_predict = [result_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres_predicted.nc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets for training and prediction\n",
    "\n",
    "We'll define a function to extract an array of variable(s) for a single level from an xarray DataSet, and another to extract both features and labels from NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def extract_data_array(dataset,\n",
    "                       variables,\n",
    "                       lev):\n",
    "\n",
    "    # allocate the array\n",
    "    arr = np.empty(shape=[dataset.time.size, \n",
    "                          dataset.lat.size, \n",
    "                          dataset.lon.size, \n",
    "                          len(variables)],\n",
    "                   dtype=np.float64)\n",
    "    \n",
    "    # for each variable we'll extract the values \n",
    "    for var_index, var in enumerate(variables):\n",
    "\n",
    "        # if we have (time, lev, lat, lon), then use level parameter\n",
    "        dimensions = dataset.variables[var].dims\n",
    "        if dimensions == ('time', 'lev', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, lev, :, :]\n",
    "        elif dimensions == ('time', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, :, :]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported variable dimensions: {dims}\".format(dims=dimensions))\n",
    "\n",
    "        # add the values into the array at the variable's position\n",
    "        arr[:, :, :, var_index] = values\n",
    "    \n",
    "    return arr\n",
    "    \n",
    "    \n",
    "def extract_features_labels(netdcf_features, \n",
    "                            netcdf_labels,\n",
    "                            feature_vars,\n",
    "                            label_vars,\n",
    "                            level=0):\n",
    "    \"\"\"\n",
    "    Extracts feature and label data from specified NetCDF files for a single level as numpy arrays.\n",
    "    \n",
    "    The feature and label NetCDFs are expected to have matching time, level, lat, and lon coordinate variables.\n",
    "    \n",
    "    Returns two arrays: the first for features and the second for labels. Arrays will have shape (time, lat, lon, var),\n",
    "    where var is the number of feature or label variables. For example if the dimensions of feature data variables in \n",
    "    the NetCDF is (time: 360, lev: 26, lat: 120, lon: 180) and the features specified are [\"T\", \"U\"] then the resulting\n",
    "    features array will have shape (360, 120, 180, 2), with the first feature variable \"T\" corresponding to array[:, :, :, 0]\n",
    "    and the second feature variable \"U\" corresponding to array[:, :, :, 1].\n",
    "    \n",
    "    :param netdcf_features: one or more NetCDF files containing feature variables, can be single file or list\n",
    "    :param netdcf_features: one or more NetCDF files containing label variables, can be single file or list\n",
    "    :param feature_vars: list of feature variable names to be extracted from the features NetCDF\n",
    "    :param label_vars: list of label variable names to be extracted from the labels NetCDF\n",
    "    :param level: index of the level to be extracted (all times/lats/lons at this level for each feature/label variable)\n",
    "    :return: two 4-D numpy arrays, the first for features and the second for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the features (flows) and labels (tendencies) as xarray DataSets\n",
    "    ds_features = xr.open_mfdataset(paths=netdcf_features)\n",
    "    ds_labels = xr.open_mfdataset(paths=netcdf_labels)\n",
    "\n",
    "    # confirm that we have datasets that match on the time, lev, lat, and lon dimension/coordinate\n",
    "    if np.any(ds_features.variables['time'].values != ds_labels.variables['time'].values):\n",
    "        raise ValueError('Non-matching time values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lev'].values != ds_labels.variables['lev'].values):\n",
    "        raise ValueError('Non-matching level values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lat'].values != ds_labels.variables['lat'].values):\n",
    "        raise ValueError('Non-matching lat values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lon'].values != ds_labels.variables['lon'].values):\n",
    "        raise ValueError('Non-matching lon values between feature and label datasets')\n",
    "\n",
    "    # extract feature and label arrays at the specified level\n",
    "    array_features = extract_data_array(ds_features, feature_vars, level)\n",
    "    array_labels = extract_data_array(ds_labels, label_vars, level)\n",
    "    \n",
    "    return array_features, array_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature variables being used are 'PS', 'T', 'U', and 'V'. The label variable is 'PTTEND'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PS\", \"T\", \"U\", \"V\"]\n",
    "labels = [\"PTTEND\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NetCDF files to load feature and label datasets that will be used for training and prediction.\n",
    "\n",
    "All files should share the same time, level, lat, and lon coordinate, with each file's feature or label variables having shape (times, levels, lats, lons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = extract_features_labels(netcdf_features_train[0],\n",
    "                                           netcdf_labels_train[0],\n",
    "                                           features,\n",
    "                                           labels,\n",
    "                                           level=0)\n",
    "predict_x = extract_data_array(xr.open_dataset(netcdf_features_predict[0]),\n",
    "                               features,\n",
    "                               lev=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimension sizes for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_times_train = train_x.shape[0]\n",
    "size_times_predict = predict_x.shape[0]\n",
    "size_lat = train_x.shape[1]\n",
    "size_lon = train_x.shape[2]\n",
    "size_lev = xr.open_dataset(netcdf_features_predict[0]).lev.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models work much better if all values are scaled into a range such as between 0 and 1. For this purpose we'll use scikit-learn's MinMaxScaler for now. The scaler being used for labels will be reused later for inverse scaling of the predicted label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize a list to store scalers for each feature/label\n",
    "scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "\n",
    "# function to perform scaling\n",
    "def scale_4d(features_train,\n",
    "             features_predict,\n",
    "             labels_train,\n",
    "             scalers_feature,\n",
    "             scalers_label):\n",
    "    \n",
    "    # make new arrays to contain the scaled values we'll return\n",
    "    scaled_features_train = np.empty(shape=features_train.shape)\n",
    "    scaled_features_predict = np.empty(shape=features_predict.shape)\n",
    "    scaled_labels_train = np.empty(shape=labels_train.shape)\n",
    "    \n",
    "    # data is 4-D with shape (times, lats, lons, vars), scalers can only work on 2-D arrays,\n",
    "    # so for each feature we scale the corresponding 3-D array of values after flattening it,\n",
    "    # then reshape back into the original shape\n",
    "    for feature_ix in range(features_train.shape[-1]):\n",
    "        scaler = scalers_feature[feature_ix]\n",
    "        feature_train = features_train[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "        feature_predict = features_predict[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "        scaled_train = scaler.fit_transform(feature_train)\n",
    "        scaled_predict = scaler.fit_transform(feature_predict)\n",
    "        reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "        reshaped_scaled_predict = np.reshape(scaled_predict, newshape=(size_times_predict, size_lat, size_lon))\n",
    "        scaled_features_train[:, :, :, feature_ix] = reshaped_scaled_train\n",
    "        scaled_features_predict[:, :, :, feature_ix] = reshaped_scaled_predict\n",
    "    for label_ix in range(len(labels)):\n",
    "        scaler = scalers_label[label_ix]\n",
    "        label_train = labels_train[:, :, :, label_ix].flatten().reshape(-1, 1)\n",
    "        scaled_train = scaler.fit_transform(label_train)\n",
    "        reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "        scaled_labels_train[:, :, :, label_ix] = reshaped_scaled_train\n",
    "    \n",
    "    # return the scaled values as well as the scalers that have been fitted to the data\n",
    "    return scaled_features_train, scaled_features_predict, scaled_labels_train, scalers_feature, scalers_label\n",
    "\n",
    "# scale the training features and labels and prediction features\n",
    "scaled_train_x, scaled_predict_x, scaled_train_y, scalers_x, scalers_y = \\\n",
    "    scale_4d(train_x, predict_x, train_y, scalers_x, scalers_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras models to use for prediciton\n",
    "We'll define two neural network models using the Keras library with TensorFlow as its backend. One of these models will contain only simple densely connected layers, and another will contain a both convolutional layer and a densely connected layer. We'll use both of these for prediction of labels corresponding to the results of NCAR CAM model runs involving computation of the Held-Suarez test case. Initially we'll focus on the input feature variables PS, T, U, and V and the output label PTTEND.\n",
    "\n",
    "##### Dense layer-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamsjam/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/adamsjam/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 69\n",
      "Trainable params: 69\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "dense_model = Sequential()\n",
    "\n",
    "# add a fully-connected hidden layer with the same number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features), input_dim=len(features), activation='relu'))\n",
    "\n",
    "# add a fully-connected hidden layer with the twice the number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "dense_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "dense_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_1 (Conv3D)            (None, 720, 12, 23, 32)   3488      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 720, 12, 23, 8)    264       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 720, 12, 23, 1)    9         \n",
      "=================================================================\n",
      "Total params: 3,761\n",
      "Trainable params: 3,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Dense\n",
    "\n",
    "# define the model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# add an initial 3-D convolutional layer\n",
    "cnn_model.add(Conv3D(filters=32,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation=\"relu\",\n",
    "                     data_format=\"channels_last\",\n",
    "                     input_shape=(size_times_train, size_lat, size_lon, len(features)),\n",
    "                     padding='same'))\n",
    "\n",
    "# add a fully-connected hidden layer with twice the number of neurons as input attributes (features)\n",
    "cnn_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "cnn_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape data for convolutional model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = (1, ) + scaled_train_x.shape\n",
    "shape_y = (1, ) + scaled_train_y.shape\n",
    "train_x_cnn = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "train_y_cnn = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "predict_x_cnn = np.reshape(scaled_predict_x, newshape=shape_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the convolutional model (for the first level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 1s - loss: 0.1256\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.1023\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0798\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0591\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0448\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0411\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0457\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0502\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2acdbc956da0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_x_cnn, train_y_cnn, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape data for dense model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = (size_times_train * size_lat * size_lon, len(features))\n",
    "shape_y = (size_times_train * size_lat * size_lon, len(labels))\n",
    "train_x_dense = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "train_y_dense = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "predict_x_dense = np.reshape(scaled_predict_x, newshape=shape_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the dense layers model (for the first level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 15s - loss: 0.0016\n",
      "Epoch 2/8\n",
      " - 14s - loss: 1.9989e-07\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.4166e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 2.2727e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.1789e-07\n",
      "Epoch 6/8\n",
      " - 13s - loss: 2.3085e-07\n",
      "Epoch 7/8\n",
      " - 13s - loss: 2.2333e-07\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.2777e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2acdbd51dba8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.fit(train_x_dense, train_y_dense, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Allocate an array to contain predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cnn = np.empty(shape=(size_times_predict, size_lev, size_lat, size_lon))\n",
    "prediction_dense = np.empty(shape=(size_times_predict, size_lev, size_lat, size_lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 174ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_y_scaled_cnn = cnn_model.predict(predict_x_cnn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 720, 12, 23, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y_scaled_cnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction loop\n",
    "We'll loop over each level, taking data for the full grid at that level (all time steps) and training the model with those feature and labels. We'll then use the fitted model to predict label values for the level using input feature values from a different time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/predicting for level 0\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0509\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0487\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0454\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0423\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0399\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0384\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0378\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0380\n",
      "Epoch 1/8\n",
      " - 14s - loss: 2.2218e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.2731e-07\n",
      "Epoch 3/8\n",
      " - 13s - loss: 2.1934e-07\n",
      "Epoch 4/8\n",
      " - 13s - loss: 2.2331e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.2162e-07\n",
      "Epoch 6/8\n",
      " - 14s - loss: 2.2407e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.0883e-07\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.1955e-07\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "198720/198720 [==============================] - 9s 43us/step\n",
      "Training/predicting for level 1\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0619\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0613\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0608\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0602\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0592\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0580\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0566\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0551\n",
      "Epoch 1/8\n",
      " - 13s - loss: 2.4501e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.3236e-07\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.3604e-07\n",
      "Epoch 4/8\n",
      " - 13s - loss: 2.3540e-07\n",
      "Epoch 5/8\n",
      " - 13s - loss: 2.3068e-07\n",
      "Epoch 6/8\n",
      " - 14s - loss: 2.3583e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.2441e-07\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.4096e-07\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "198720/198720 [==============================] - 10s 50us/step\n",
      "Training/predicting for level 2\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0761\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0729\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0698\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0671\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0649\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0634\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0621\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0606\n",
      "Epoch 1/8\n",
      " - 13s - loss: 2.2135e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.2187e-07\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.2454e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 2.1637e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.2300e-07\n",
      "Epoch 6/8\n",
      " - 14s - loss: 2.3456e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.1563e-07\n",
      "Epoch 8/8\n",
      " - 13s - loss: 2.2260e-07\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "198720/198720 [==============================] - 8s 39us/step\n",
      "Training/predicting for level 3\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0514\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0494\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0474\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0454\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0434\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0415\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0395\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0377\n",
      "Epoch 1/8\n",
      " - 14s - loss: 1.9669e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.1219e-07\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.1568e-07\n",
      "Epoch 4/8\n",
      " - 13s - loss: 2.0338e-07\n",
      "Epoch 5/8\n",
      " - 13s - loss: 1.9682e-07\n",
      "Epoch 6/8\n",
      " - 13s - loss: 1.9965e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 1.9754e-07\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.0383e-07\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "198720/198720 [==============================] - 10s 49us/step\n",
      "Training/predicting for level 4\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0410\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0387\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0364\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0342\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0322\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0304\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0286\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0268\n",
      "Epoch 1/8\n",
      " - 13s - loss: 2.6379e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 1.9643e-07\n",
      "Epoch 3/8\n",
      " - 13s - loss: 2.4227e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 2.0707e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.2454e-07\n",
      "Epoch 6/8\n",
      " - 15s - loss: 2.0739e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.2161e-07\n",
      "Epoch 8/8\n",
      " - 15s - loss: 2.0487e-07\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "198720/198720 [==============================] - 13s 64us/step\n",
      "Training/predicting for level 5\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0251\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0236\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0221\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0208\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0196\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0187\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0178\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0169\n",
      "Epoch 1/8\n",
      " - 15s - loss: 2.0076e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.0407e-07\n",
      "Epoch 3/8\n",
      " - 15s - loss: 2.1593e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 1.9700e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.1148e-07\n",
      "Epoch 6/8\n",
      " - 15s - loss: 2.1004e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.0689e-07\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.1250e-07\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "198720/198720 [==============================] - 10s 52us/step\n",
      "Training/predicting for level 6\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0144\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0135\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0126\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0119\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0114\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0110\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0106\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0102\n",
      "Epoch 1/8\n",
      " - 14s - loss: 2.4969e-07\n",
      "Epoch 2/8\n",
      " - 15s - loss: 2.4057e-07\n",
      "Epoch 3/8\n",
      " - 15s - loss: 2.4078e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 2.4401e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.4730e-07\n",
      "Epoch 6/8\n",
      " - 14s - loss: 2.3953e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.4308e-07\n",
      "Epoch 8/8\n",
      " - 13s - loss: 2.3727e-07\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "198720/198720 [==============================] - 9s 44us/step\n",
      "Training/predicting for level 7\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0100\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0092\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0075\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0072\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 1/8\n",
      " - 14s - loss: 5.1758e-07\n",
      "Epoch 2/8\n",
      " - 14s - loss: 4.4710e-07\n",
      "Epoch 3/8\n",
      " - 14s - loss: 4.3833e-07\n",
      "Epoch 4/8\n",
      " - 14s - loss: 4.4587e-07\n",
      "Epoch 5/8\n",
      " - 14s - loss: 4.3903e-07\n",
      "Epoch 6/8\n",
      " - 13s - loss: 4.2710e-07\n",
      "Epoch 7/8\n",
      " - 14s - loss: 4.3306e-07\n",
      "Epoch 8/8\n",
      " - 15s - loss: 4.4609e-07\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "198720/198720 [==============================] - 9s 44us/step\n",
      "Training/predicting for level 8\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0051\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0045\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 1/8\n",
      " - 14s - loss: 1.3628e-05\n",
      "Epoch 2/8\n",
      " - 14s - loss: 8.1802e-06\n",
      "Epoch 3/8\n",
      " - 14s - loss: 8.1227e-06\n",
      "Epoch 4/8\n",
      " - 14s - loss: 8.1389e-06\n",
      "Epoch 5/8\n",
      " - 14s - loss: 8.1768e-06\n",
      "Epoch 6/8\n",
      " - 14s - loss: 8.1193e-06\n",
      "Epoch 7/8\n",
      " - 14s - loss: 8.0167e-06\n",
      "Epoch 8/8\n",
      " - 14s - loss: 8.1601e-06\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "198720/198720 [==============================] - 14s 72us/step\n",
      "Training/predicting for level 9\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0320\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0257\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0177\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0108\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0061\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0034\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 1/8\n",
      " - 14s - loss: 1.3943e-04\n",
      "Epoch 2/8\n",
      " - 15s - loss: 6.2388e-05\n",
      "Epoch 3/8\n",
      " - 14s - loss: 6.2592e-05\n",
      "Epoch 4/8\n",
      " - 14s - loss: 6.2473e-05\n",
      "Epoch 5/8\n",
      " - 14s - loss: 6.2414e-05\n",
      "Epoch 6/8\n",
      " - 14s - loss: 6.2494e-05\n",
      "Epoch 7/8\n",
      " - 15s - loss: 6.2324e-05\n",
      "Epoch 8/8\n",
      " - 15s - loss: 6.1985e-05\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "198720/198720 [==============================] - 11s 55us/step\n",
      "Training/predicting for level 10\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0073\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0057\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0044\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 8/8\n",
      " - 0s - loss: 7.2786e-04\n",
      "Epoch 1/8\n",
      " - 15s - loss: 1.5547e-04\n",
      "Epoch 2/8\n",
      " - 15s - loss: 1.3180e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 1.3113e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 1.2977e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 1.3006e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 1.2944e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 1.2896e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 1.2912e-04\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "198720/198720 [==============================] - 11s 54us/step\n",
      "Training/predicting for level 11\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 3/8\n",
      " - 0s - loss: 8.2975e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 6.2019e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.6653e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 6.1653e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 7.3258e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 8.7354e-04\n",
      "Epoch 1/8\n",
      " - 14s - loss: 1.8606e-04\n",
      "Epoch 2/8\n",
      " - 13s - loss: 1.8435e-04\n",
      "Epoch 3/8\n",
      " - 13s - loss: 1.8379e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 1.8386e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 1.8356e-04\n",
      "Epoch 6/8\n",
      " - 15s - loss: 1.8321e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 1.8298e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 1.8287e-04\n",
      "1/1 [==============================] - 0s 54ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198720/198720 [==============================] - 9s 46us/step\n",
      "Training/predicting for level 12\n",
      "Epoch 1/8\n",
      " - 0s - loss: 5.7236e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 6.0007e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 6.2416e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 6.3452e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 6.2659e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 6.0059e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 5.6008e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 5.1061e-04\n",
      "Epoch 1/8\n",
      " - 13s - loss: 2.2598e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.2484e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.2451e-04\n",
      "Epoch 4/8\n",
      " - 13s - loss: 2.2429e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.2415e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 2.2404e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.2395e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.2364e-04\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "198720/198720 [==============================] - 12s 62us/step\n",
      "Training/predicting for level 13\n",
      "Epoch 1/8\n",
      " - 0s - loss: 4.1136e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 3.8507e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 3.6669e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 3.5634e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 3.5265e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 3.5362e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 3.5726e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 3.6178e-04\n",
      "Epoch 1/8\n",
      " - 13s - loss: 2.7377e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 2.7340e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 2.7337e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 2.7311e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 2.7294e-04\n",
      "Epoch 6/8\n",
      " - 13s - loss: 2.7286e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 2.7269e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 2.7250e-04\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "198720/198720 [==============================] - 12s 62us/step\n",
      "Training/predicting for level 14\n",
      "Epoch 1/8\n",
      " - 0s - loss: 4.7341e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 4.7475e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 4.7297e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 4.6836e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 4.6157e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 4.5344e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 4.4484e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 4.3660e-04\n",
      "Epoch 1/8\n",
      " - 14s - loss: 3.3491e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 3.3436e-04\n",
      "Epoch 3/8\n",
      " - 15s - loss: 3.3380e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 3.3310e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 3.3241e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 3.3168e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 3.3127e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 3.3051e-04\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "198720/198720 [==============================] - 13s 68us/step\n",
      "Training/predicting for level 15\n",
      "Epoch 1/8\n",
      " - 0s - loss: 5.4994e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 5.3754e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 5.2657e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 5.1777e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.1147e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 5.0761e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 5.0584e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 5.0557e-04\n",
      "Epoch 1/8\n",
      " - 14s - loss: 4.0701e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 4.0648e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 4.0688e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 4.0599e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 4.0587e-04\n",
      "Epoch 6/8\n",
      " - 13s - loss: 4.0537e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 4.0515e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 4.0445e-04\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "198720/198720 [==============================] - 12s 59us/step\n",
      "Training/predicting for level 16\n",
      "Epoch 1/8\n",
      " - 0s - loss: 5.7491e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 5.7562e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 5.7616e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 5.7616e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.7543e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 5.7393e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 5.7178e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 5.6917e-04\n",
      "Epoch 1/8\n",
      " - 14s - loss: 4.9320e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 4.9263e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 4.9284e-04\n",
      "Epoch 4/8\n",
      " - 13s - loss: 4.9271e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 4.9212e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 4.9162e-04\n",
      "Epoch 7/8\n",
      " - 13s - loss: 4.9111e-04\n",
      "Epoch 8/8\n",
      " - 17s - loss: 4.8964e-04\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "198720/198720 [==============================] - 15s 74us/step\n",
      "Training/predicting for level 17\n",
      "Epoch 1/8\n",
      " - 0s - loss: 6.0118e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 5.9609e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 5.9076e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 5.8583e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.8176e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 5.7881e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 5.7701e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 5.7623e-04\n",
      "Epoch 1/8\n",
      " - 19s - loss: 5.2566e-04\n",
      "Epoch 2/8\n",
      " - 21s - loss: 5.2443e-04\n",
      "Epoch 3/8\n",
      " - 20s - loss: 5.2404e-04\n",
      "Epoch 4/8\n",
      " - 20s - loss: 5.2365e-04\n",
      "Epoch 5/8\n",
      " - 19s - loss: 5.2321e-04\n",
      "Epoch 6/8\n",
      " - 19s - loss: 5.2348e-04\n",
      "Epoch 7/8\n",
      " - 20s - loss: 5.2307e-04\n",
      "Epoch 8/8\n",
      " - 20s - loss: 5.2286e-04\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "198720/198720 [==============================] - 15s 76us/step\n",
      "Training/predicting for level 18\n",
      "Epoch 1/8\n",
      " - 0s - loss: 6.0434e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 6.0431e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 6.0454e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 6.0479e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 6.0484e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 6.0457e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 6.0392e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 6.0294e-04\n",
      "Epoch 1/8\n",
      " - 20s - loss: 5.4597e-04\n",
      "Epoch 2/8\n",
      " - 20s - loss: 5.4357e-04\n",
      "Epoch 3/8\n",
      " - 17s - loss: 5.4276e-04\n",
      "Epoch 4/8\n",
      " - 13s - loss: 5.4259e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 5.4237e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 5.4247e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 5.4177e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 5.4178e-04\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "198720/198720 [==============================] - 19s 94us/step\n",
      "Training/predicting for level 19\n",
      "Epoch 1/8\n",
      " - 0s - loss: 6.3893e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 6.3826e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 6.3757e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 6.3685e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 6.3611e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 6.3534e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 6.3456e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 6.3378e-04\n",
      "Epoch 1/8\n",
      " - 13s - loss: 5.6994e-04\n",
      "Epoch 2/8\n",
      " - 14s - loss: 5.6935e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 5.6827e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 5.6837e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 5.6847e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 5.6853e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 5.6823e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 5.6866e-04\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "198720/198720 [==============================] - 18s 92us/step\n",
      "Training/predicting for level 20\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0011\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 7/8\n",
      " - 0s - loss: 9.9551e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 9.7087e-04\n",
      "Epoch 1/8\n",
      " - 14s - loss: 8.4574e-04\n",
      "Epoch 2/8\n",
      " - 13s - loss: 8.3969e-04\n",
      "Epoch 3/8\n",
      " - 14s - loss: 8.3855e-04\n",
      "Epoch 4/8\n",
      " - 14s - loss: 8.3815e-04\n",
      "Epoch 5/8\n",
      " - 14s - loss: 8.3763e-04\n",
      "Epoch 6/8\n",
      " - 14s - loss: 8.3580e-04\n",
      "Epoch 7/8\n",
      " - 14s - loss: 8.3612e-04\n",
      "Epoch 8/8\n",
      " - 14s - loss: 8.3661e-04\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "198720/198720 [==============================] - 17s 87us/step\n",
      "Training/predicting for level 21\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0046\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0028\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0024\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0021\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0020\n",
      "Epoch 1/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 2/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 3/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 4/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 5/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 6/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 7/8\n",
      " - 14s - loss: 0.0015\n",
      "Epoch 8/8\n",
      " - 13s - loss: 0.0015\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "198720/198720 [==============================] - 17s 87us/step\n",
      "Training/predicting for level 22\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0036\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 1/8\n",
      " - 13s - loss: 0.0025\n",
      "Epoch 2/8\n",
      " - 14s - loss: 0.0025\n",
      "Epoch 3/8\n",
      " - 14s - loss: 0.0025\n",
      "Epoch 4/8\n",
      " - 13s - loss: 0.0025\n",
      "Epoch 5/8\n",
      " - 14s - loss: 0.0025\n",
      "Epoch 6/8\n",
      " - 14s - loss: 0.0025\n",
      "Epoch 7/8\n",
      " - 14s - loss: 0.0025\n",
      "Epoch 8/8\n",
      " - 14s - loss: 0.0025\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "198720/198720 [==============================] - 18s 93us/step\n",
      "Training/predicting for level 23\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0234\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0217\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0196\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0172\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0147\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0123\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0101\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 1/8\n",
      " - 14s - loss: 0.0057\n",
      "Epoch 2/8\n",
      " - 14s - loss: 0.0056\n",
      "Epoch 3/8\n",
      " - 14s - loss: 0.0056\n",
      "Epoch 4/8\n",
      " - 14s - loss: 0.0056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      " - 14s - loss: 0.0056\n",
      "Epoch 6/8\n",
      " - 14s - loss: 0.0054\n",
      "Epoch 7/8\n",
      " - 14s - loss: 0.0052\n",
      "Epoch 8/8\n",
      " - 14s - loss: 0.0052\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "198720/198720 [==============================] - 18s 92us/step\n",
      "Training/predicting for level 24\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0346\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0282\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0221\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0169\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0129\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0105\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0098\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0106\n",
      "Epoch 1/8\n",
      " - 13s - loss: 0.0081\n",
      "Epoch 2/8\n",
      " - 13s - loss: 0.0080\n",
      "Epoch 3/8\n",
      " - 13s - loss: 0.0080\n",
      "Epoch 4/8\n",
      " - 13s - loss: 0.0080\n",
      "Epoch 5/8\n",
      " - 13s - loss: 0.0080\n",
      "Epoch 6/8\n",
      " - 14s - loss: 0.0080\n",
      "Epoch 7/8\n",
      " - 14s - loss: 0.0079\n",
      "Epoch 8/8\n",
      " - 14s - loss: 0.0078\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "198720/198720 [==============================] - ETA:  - 18s 92us/step\n",
      "Training/predicting for level 25\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0087\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0105\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0124\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0126\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0122\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0114\n",
      "Epoch 1/8\n",
      " - 14s - loss: 0.0058\n",
      "Epoch 2/8\n",
      " - 14s - loss: 0.0056\n",
      "Epoch 3/8\n",
      " - 13s - loss: 0.0053\n",
      "Epoch 4/8\n",
      " - 13s - loss: 0.0052\n",
      "Epoch 5/8\n",
      " - 14s - loss: 0.0052\n",
      "Epoch 6/8\n",
      " - 14s - loss: 0.0052\n",
      "Epoch 7/8\n",
      " - 15s - loss: 0.0052\n",
      "Epoch 8/8\n",
      " - 14s - loss: 0.0052\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "198720/198720 [==============================] - 17s 85us/step\n"
     ]
    }
   ],
   "source": [
    "# loop over all levels\n",
    "for lev in range(size_lev):\n",
    "    \n",
    "    print(\"Training/predicting for level {level}\".format(level=lev))\n",
    "    \n",
    "    # get the features and labels for training\n",
    "    train_x, train_y = extract_features_labels(netcdf_features_train[0],\n",
    "                                               netcdf_labels_train[0],\n",
    "                                               features,\n",
    "                                               labels,\n",
    "                                               lev)\n",
    "    \n",
    "    # get the features for prediction\n",
    "    predict_x = extract_data_array(xr.open_dataset(netcdf_features_predict[0]),\n",
    "                                   features,\n",
    "                                   lev)\n",
    "\n",
    "    # scale the data between 0 and 1\n",
    "    scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "    scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "    scaled_train_x, scaled_predict_x, scaled_train_y, scalers_x, scalers_y = \\\n",
    "        scale_4d(train_x, predict_x, train_y, scalers_x, scalers_y)\n",
    "    \n",
    "    # reshape the data for convolutional model input\n",
    "    shape_x = (1, ) + scaled_train_x.shape\n",
    "    shape_y = (1, ) + scaled_train_y.shape\n",
    "    train_x_cnn = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "    train_y_cnn = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "    predict_x_cnn = np.reshape(scaled_predict_x, newshape=shape_x)\n",
    "    \n",
    "    # reshape the data for dense layer model input\n",
    "    shape_x = (size_times_train * size_lat * size_lon, len(features))\n",
    "    shape_y = (size_times_train * size_lat * size_lon, len(labels))\n",
    "    train_x_dense = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "    train_y_dense = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "    predict_x_dense = np.reshape(scaled_predict_x, newshape=shape_x)\n",
    "    \n",
    "    # train the models\n",
    "    cnn_model.fit(train_x_cnn, train_y_cnn, shuffle=True, epochs=8, verbose=2)\n",
    "    dense_model.fit(train_x_dense, train_y_dense, shuffle=True, epochs=8, verbose=2)\n",
    "    \n",
    "    # use the fitted models to make predictions\n",
    "    predict_y_scaled_cnn = cnn_model.predict(predict_x_cnn, verbose=1)\n",
    "    predict_y_scaled_dense = dense_model.predict(predict_x_dense, verbose=1)\n",
    "\n",
    "    # reverse the scaling of the predicted values\n",
    "    # TODO below assumes a single label, will need modification for multiple labels\n",
    "    scaler = scalers_y[0]  # assumes the label scaler was fitted in scale_4d() and side effect carried through\n",
    "        \n",
    "    # output from the dense model is 2-D, good for scaler input\n",
    "    unscaled_predict_y_dense = scaler.inverse_transform(predict_y_scaled_dense)\n",
    "        \n",
    "    # output from CNN model is 5-D, so we'll flatten first to make it amenable to scaling\n",
    "    unscaled_predict_y_cnn = scaler.inverse_transform(predict_y_scaled_cnn.flatten().reshape(-1, 1))\n",
    "    \n",
    "    # reshape data so it's compatible with assignment into prediction arrays\n",
    "    level_shape = (size_times_predict, size_lat, size_lon)\n",
    "    prediction_cnn[:, lev, :, :] = np.reshape(unscaled_predict_y_cnn, newshape=level_shape)\n",
    "    prediction_dense[:, lev, :, :] = np.reshape(unscaled_predict_y_dense, newshape=level_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of predicted values as NetCDF\n",
    "At this point the entire dataset has been predicted and the predicted values are in arrays that we can add to an xarray DataSet that we'll then write as NetCDF. We will first make a copy of the prediction features dataset since this has all the coordinate variables and attributes we'll want in the predicted labels dataset, we'll then remove all data variables before adding the predicted values arrays as new variables to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the prediction features dataset since the predicted label(s) should share the same coordinates, etc.\n",
    "ds_predict_labels = xr.open_dataset(data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres.nc\")\n",
    "\n",
    "# remove all non-label data variables from the predictions dataset\n",
    "for var in ds_predict_labels.data_vars:\n",
    "    if var not in labels:\n",
    "        ds_predict_labels = ds_predict_labels.drop(var)\n",
    "\n",
    "# create new variables to contain the predicted labels, assign these into the prediction dataset\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=prediction_cnn,\n",
    "                                  attrs=ds_predict_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_cnn\"] = predicted_label_var\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=prediction_dense,\n",
    "                                  attrs=ds_predict_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_dense\"] = predicted_label_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create variables to contain the differences between predicted values and those computed by NCAR CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the dataset containing the computed label values corresponding to the input features used for prediction\n",
    "ds_cam_labels = xr.open_dataset(data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres.nc\")\n",
    "\n",
    "# get the differences between computed and predicted values\n",
    "pttend_diff_cnn = ds_cam_labels[labels[0]].values - prediction_cnn\n",
    "pttend_diff_dense = ds_cam_labels[labels[0]].values - prediction_dense\n",
    "\n",
    "# create the variables and add to the dataset\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=pttend_diff_cnn,\n",
    "                                  attrs=ds_predict_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_cnn_diff\"] = predicted_label_var\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=pttend_diff_dense,\n",
    "                                  attrs=ds_predict_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_dense_diff\"] = predicted_label_var\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output predicted dataset as NetCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the predicted label(s)' dataset as a NetCDF file\n",
    "ds_predict_labels.to_netcdf(netcdf_predict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot difference for various times/levels\n",
    "We'll use xarray's simple plotting functions to illustrate some of the differences we see at various times and levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_dense = ds_predict_labels[labels[0] + \"_dense_diff\"].isel(time=0, lev=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x2ace3be8cfd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "diff_dense.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
