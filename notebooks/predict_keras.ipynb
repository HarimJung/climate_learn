{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify NetCDF files used for training and prediciton inputs\n",
    "These are low resolution versions of NCAR CAM inputs/outputs, located in the `example_data` directory of this project's git repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/home/adamsjam/git/model_learn/example_data\"\n",
    "result_dir = \"/home/adamsjam/test\"\n",
    "\n",
    "# files used as feature inputs for model training\n",
    "netcdf_features_train = [data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-01-11-00000_lowres.nc\",\n",
    "                         data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-01-26-00000_lowres.nc\",\n",
    "                         data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label inputs for model training\n",
    "netcdf_labels_train = [data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-01-11-00000_lowres.nc\",\n",
    "                       data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-01-26-00000_lowres.nc\",\n",
    "                       data_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-10-00000_lowres.nc\"]\n",
    "\n",
    "# files used as feature inputs for model prediction\n",
    "netcdf_features_predict = [data_dir + \"/fv091x180L26_moist_HS.cam.h0.2001-02-25-00000_lowres.nc\"]\n",
    "\n",
    "# files used as label outputs for model prediction\n",
    "netcdf_predict = [result_dir + \"/fv091x180L26_moist_HS.cam.h1.2001-02-25-00000_lowres_predicted.nc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets for training and prediction\n",
    "\n",
    "We'll define a function to extract an array of variable(s) for a single level from an xarray DataSet, and another to extract both features and labels from NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "def extract_data_array(dataset,\n",
    "                       variables,\n",
    "                       lev):\n",
    "\n",
    "    # allocate the array\n",
    "    arr = np.empty(shape=[dataset.time.size, \n",
    "                          dataset.lat.size, \n",
    "                          dataset.lon.size, \n",
    "                          len(variables)],\n",
    "                   dtype=np.float64)\n",
    "    \n",
    "    # for each variable we'll extract the values \n",
    "    for var_index, var in enumerate(variables):\n",
    "\n",
    "        # if we have (time, lev, lat, lon), then use level parameter\n",
    "        dimensions = dataset.variables[var].dims\n",
    "        if dimensions == ('time', 'lev', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, lev, :, :]\n",
    "        elif dimensions == ('time', 'lat', 'lon'):\n",
    "            values = dataset[var].values[:, :, :]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported variable dimensions: {dims}\".format(dims=dimensions))\n",
    "\n",
    "        # add the values into the array at the variable's position\n",
    "        arr[:, :, :, var_index] = values\n",
    "    \n",
    "    return arr\n",
    "    \n",
    "    \n",
    "def extract_features_labels(netdcf_features, \n",
    "                            netcdf_labels,\n",
    "                            feature_vars,\n",
    "                            label_vars,\n",
    "                            level=0):\n",
    "    \"\"\"\n",
    "    Extracts feature and label data from specified NetCDF files for a single level as numpy arrays.\n",
    "    \n",
    "    The feature and label NetCDFs are expected to have matching time, level, lat, and lon coordinate variables.\n",
    "    \n",
    "    Returns two arrays: the first for features and the second for labels. Arrays will have shape (time, lat, lon, var),\n",
    "    where var is the number of feature or label variables. For example if the dimensions of feature data variables in \n",
    "    the NetCDF is (time: 360, lev: 26, lat: 120, lon: 180) and the features specified are [\"T\", \"U\"] then the resulting\n",
    "    features array will have shape (360, 120, 180, 2), with the first feature variable \"T\" corresponding to array[:, :, :, 0]\n",
    "    and the second feature variable \"U\" corresponding to array[:, :, :, 1].\n",
    "    \n",
    "    :param netdcf_features: one or more NetCDF files containing feature variables, can be single file or list\n",
    "    :param netdcf_features: one or more NetCDF files containing label variables, can be single file or list\n",
    "    :param feature_vars: list of feature variable names to be extracted from the features NetCDF\n",
    "    :param label_vars: list of label variable names to be extracted from the labels NetCDF\n",
    "    :param level: index of the level to be extracted (all times/lats/lons at this level for each feature/label variable)\n",
    "    :return: two 4-D numpy arrays, the first for features and the second for labels\n",
    "    \"\"\"\n",
    "    \n",
    "    # open the features (flows) and labels (tendencies) as xarray DataSets\n",
    "    ds_features = xr.open_mfdataset(paths=netdcf_features)\n",
    "    ds_labels = xr.open_mfdataset(paths=netcdf_labels)\n",
    "\n",
    "    # confirm that we have datasets that match on the time, lev, lat, and lon dimension/coordinate\n",
    "    if np.any(ds_features.variables['time'].values != ds_labels.variables['time'].values):\n",
    "        raise ValueError('Non-matching time values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lev'].values != ds_labels.variables['lev'].values):\n",
    "        raise ValueError('Non-matching level values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lat'].values != ds_labels.variables['lat'].values):\n",
    "        raise ValueError('Non-matching lat values between feature and label datasets')\n",
    "    if np.any(ds_features.variables['lon'].values != ds_labels.variables['lon'].values):\n",
    "        raise ValueError('Non-matching lon values between feature and label datasets')\n",
    "\n",
    "    # extract feature and label arrays at the specified level\n",
    "    array_features = extract_data_array(ds_features, feature_vars, level)\n",
    "    array_labels = extract_data_array(ds_labels, label_vars, level)\n",
    "    \n",
    "    return array_features, array_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature variables being used are 'PS', 'T', 'U', and 'V'. The label variable is 'PTTEND'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"PS\", \"T\", \"U\", \"V\"]\n",
    "labels = [\"PTTEND\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read NetCDF files to load feature and label datasets that will be used for training and prediction.\n",
    "\n",
    "All files should share the same time, level, lat, and lon coordinate, with each file's feature or label variables having shape (times, levels, lats, lons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = extract_features_labels(netcdf_features_train[0],\n",
    "                                           netcdf_labels_train[0],\n",
    "                                           features,\n",
    "                                           labels,\n",
    "                                           level=0)\n",
    "predict_x = extract_data_array(xr.open_dataset(netcdf_features_predict[0]),\n",
    "                               features,\n",
    "                               lev=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the dimension sizes for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_times_train = train_x.shape[0]\n",
    "size_times_predict = predict_x.shape[0]\n",
    "size_lat = train_x.shape[1]\n",
    "size_lon = train_x.shape[2]\n",
    "size_lev = xr.open_dataset(netcdf_features_predict[0]).lev.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network models work much better if all values are scaled into a range such as between 0 and 1. For this purpose we'll use scikit-learn's MinMaxScaler for now. The scaler being used for labels will be reused later for inverse scaling of the predicted label values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize a list to store scalers for each feature/label\n",
    "scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "\n",
    "# function to perform scaling\n",
    "def scale_4d(features_train,\n",
    "             features_predict,\n",
    "             labels_train,\n",
    "             feature_scalers,\n",
    "             label_scalers):\n",
    "    \n",
    "    # make new arrays to contain the scaled values we'll return\n",
    "    scaled_features_train = np.empty(shape=features_train.shape)\n",
    "    scaled_features_predict = np.empty(shape=features_predict.shape)\n",
    "    scaled_labels_train = np.empty(shape=labels_train.shape)\n",
    "    \n",
    "    # data is 4-D with shape (times, lats, lons, vars), scalers can only work on 2-D arrays,\n",
    "    # so for each feature we scale the corresponding 3-D array of values after flattening it,\n",
    "    # then reshape back into the original shape\n",
    "    for feature_ix in range(features_train.shape[-1]):\n",
    "        scaler = feature_scalers[feature_ix]\n",
    "        feature_train = features_train[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "        feature_predict = features_predict[:, :, :, feature_ix].flatten().reshape(-1, 1)\n",
    "        scaled_train = scaler.fit_transform(feature_train)\n",
    "        scaled_predict = scaler.fit_transform(feature_predict)\n",
    "        reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "        reshaped_scaled_predict = np.reshape(scaled_predict, newshape=(size_times_predict, size_lat, size_lon))\n",
    "        scaled_features_train[:, :, :, feature_ix] = reshaped_scaled_train\n",
    "        scaled_features_predict[:, :, :, feature_ix] = reshaped_scaled_predict\n",
    "    for label_ix in range(len(labels)):\n",
    "        scaler = label_scalers[label_ix]\n",
    "        label_train = labels_train[:, :, :, label_ix].flatten().reshape(-1, 1)\n",
    "        scaled_train = scaler.fit_transform(label_train)\n",
    "        reshaped_scaled_train = np.reshape(scaled_train, newshape=(size_times_train, size_lat, size_lon))\n",
    "        scaled_labels_train[:, :, :, label_ix] = reshaped_scaled_train\n",
    "        \n",
    "    return scaled_features_train, scaled_features_predict, scaled_labels_train\n",
    "\n",
    "# scale the training features and labels and prediction features\n",
    "scaled_train_x, scaled_predict_x, scaled_train_y = scale_4d(train_x, predict_x, train_y, scalers_x, scalers_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Keras models to use for prediciton\n",
    "We'll define two neural network models using the Keras library with TensorFlow as its backend. One of these models will contain only simple densely connected layers, and another will contain a both convolutional layer and a densely connected layer. We'll use both of these for prediction of labels corresponding to the results of NCAR CAM model runs involving computation of the Held-Suarez test case. Initially we'll focus on the input feature variables PS, T, U, and V and the output label PTTEND.\n",
    "\n",
    "##### Dense layer-only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 69\n",
      "Trainable params: 69\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "dense_model = Sequential()\n",
    "\n",
    "# add a fully-connected hidden layer with the same number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features), input_dim=len(features), activation='relu'))\n",
    "\n",
    "# add a fully-connected hidden layer with the twice the number of neurons as input attributes (features)\n",
    "dense_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "dense_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "dense_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "dense_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convolutional layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_2 (Conv3D)            (None, 720, 12, 23, 32)   3488      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 720, 12, 23, 8)    264       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 720, 12, 23, 1)    9         \n",
      "=================================================================\n",
      "Total params: 3,761\n",
      "Trainable params: 3,761\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, Dense\n",
    "\n",
    "# define the model\n",
    "cnn_model = Sequential()\n",
    "\n",
    "# add an initial 3-D convolutional layer\n",
    "cnn_model.add(Conv3D(filters=32,\n",
    "                     kernel_size=(3, 3, 3),\n",
    "                     activation=\"relu\",\n",
    "                     data_format=\"channels_last\",\n",
    "                     input_shape=(size_times_train, size_lat, size_lon, len(features)),\n",
    "                     padding='same'))\n",
    "\n",
    "# add a fully-connected hidden layer with twice the number of neurons as input attributes (features)\n",
    "cnn_model.add(Dense(len(features) * 2, activation='relu'))\n",
    "\n",
    "# output layer uses no activation function since we are interested\n",
    "# in predicting numerical values directly without transform\n",
    "cnn_model.add(Dense(len(labels)))\n",
    "\n",
    "# compile the model using the ADAM optimization algorithm and a mean squared error loss function\n",
    "cnn_model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# display some summary information\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape data for convolutional model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = (1, ) + scaled_train_x.shape\n",
    "shape_y = (1, ) + scaled_train_y.shape\n",
    "train_x_cnn = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "train_y_cnn = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "predict_x_cnn = np.reshape(scaled_predict_x, newshape=shape_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the convolutional model (for the first level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 0s - loss: 0.0261\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0233\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0226\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0226\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0213\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0200\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0201\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b77a49e77b8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model.fit(train_x_cnn, train_y_cnn, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reshape data for dense model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = (size_times_train * size_lat * size_lon, len(features))\n",
    "shape_y = (size_times_train * size_lat * size_lon, len(labels))\n",
    "train_x_dense = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "train_y_dense = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "predict_x_dense = np.reshape(scaled_predict_x, newshape=shape_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train the dense layers model (for the first level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 16s - loss: 9.3371e-08\n",
      "Epoch 2/8\n",
      " - 16s - loss: 9.5670e-08\n",
      "Epoch 3/8\n",
      " - 17s - loss: 1.0026e-07\n",
      "Epoch 4/8\n",
      " - 17s - loss: 9.2125e-08\n",
      "Epoch 5/8\n",
      " - 16s - loss: 9.3517e-08\n",
      "Epoch 6/8\n",
      " - 17s - loss: 9.3825e-08\n",
      "Epoch 7/8\n",
      " - 17s - loss: 9.3823e-08\n",
      "Epoch 8/8\n",
      " - 17s - loss: 9.1776e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2b77a4929cc0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_model.fit(train_x_dense, train_y_dense, shuffle=True, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Allocate an array to contain predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_cnn = np.empty(shape=(size_times_predict, size_lev, size_lat, size_lon))\n",
    "prediction_dense = np.empty(shape=(size_times_predict, size_lev, size_lat, size_lon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 64ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_y_scaled_cnn = cnn_model.predict(predict_x_cnn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 720, 12, 23, 1)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y_scaled_cnn.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction loop\n",
    "We'll loop over each level, taking data for the full grid at that level (all time steps) and training the model with those feature and labels. We'll then use the fitted model to predict label values for the level using input feature values from a different time series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/predicting for level 0\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0176\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0167\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0157\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0156\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0144\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0144\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0133\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0133\n",
      "Epoch 1/8\n",
      " - 18s - loss: 9.8282e-08\n",
      "Epoch 2/8\n",
      " - 17s - loss: 9.0407e-08\n",
      "Epoch 3/8\n",
      " - 17s - loss: 9.3840e-08\n",
      "Epoch 4/8\n",
      " - 17s - loss: 9.9851e-08\n",
      "Epoch 5/8\n",
      " - 18s - loss: 9.0942e-08\n",
      "Epoch 6/8\n",
      " - 16s - loss: 9.1971e-08\n",
      "Epoch 7/8\n",
      " - 16s - loss: 9.4488e-08\n",
      "Epoch 8/8\n",
      " - 17s - loss: 9.0844e-08\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "198720/198720 [==============================] - 19s 96us/step\n",
      "Training/predicting for level 1\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0212\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0119\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0194\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0192\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0122\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0116\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0163\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0118\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.2190e-07\n",
      "Epoch 2/8\n",
      " - 18s - loss: 1.0547e-07\n",
      "Epoch 3/8\n",
      " - 17s - loss: 1.0149e-07\n",
      "Epoch 4/8\n",
      " - 18s - loss: 1.0598e-07\n",
      "Epoch 5/8\n",
      " - 17s - loss: 9.3919e-08\n",
      "Epoch 6/8\n",
      " - 16s - loss: 9.8941e-08\n",
      "Epoch 7/8\n",
      " - 17s - loss: 9.8787e-08\n",
      "Epoch 8/8\n",
      " - 17s - loss: 1.0061e-07\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "198720/198720 [==============================] - 20s 99us/step\n",
      "Training/predicting for level 2\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0082\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0107\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0119\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0093\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0079\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0099\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0073\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.5187e-07\n",
      "Epoch 2/8\n",
      " - 18s - loss: 1.1255e-07\n",
      "Epoch 3/8\n",
      " - 17s - loss: 1.0655e-07\n",
      "Epoch 4/8\n",
      " - 17s - loss: 9.8070e-08\n",
      "Epoch 5/8\n",
      " - 16s - loss: 1.0271e-07\n",
      "Epoch 6/8\n",
      " - 16s - loss: 9.9886e-08\n",
      "Epoch 7/8\n",
      " - 16s - loss: 1.0747e-07\n",
      "Epoch 8/8\n",
      " - 17s - loss: 1.0678e-07\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "198720/198720 [==============================] - 20s 101us/step\n",
      "Training/predicting for level 3\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0095\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0094\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0072\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0059\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0069\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0074\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0056\n",
      "Epoch 1/8\n",
      " - 16s - loss: 9.5500e-08\n",
      "Epoch 2/8\n",
      " - 17s - loss: 1.0153e-07\n",
      "Epoch 3/8\n",
      " - 18s - loss: 9.5811e-08\n",
      "Epoch 4/8\n",
      " - 18s - loss: 1.1310e-07\n",
      "Epoch 5/8\n",
      " - 17s - loss: 8.8295e-08\n",
      "Epoch 6/8\n",
      " - 17s - loss: 1.0123e-07\n",
      "Epoch 7/8\n",
      " - 17s - loss: 9.1329e-08\n",
      "Epoch 8/8\n",
      " - 16s - loss: 9.5602e-08\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "198720/198720 [==============================] - 19s 93us/step\n",
      "Training/predicting for level 4\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0067\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0066\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0055\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0052\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.2540e-07\n",
      "Epoch 2/8\n",
      " - 18s - loss: 9.4210e-08\n",
      "Epoch 3/8\n",
      " - 17s - loss: 9.7094e-08\n",
      "Epoch 4/8\n",
      " - 18s - loss: 9.7069e-08\n",
      "Epoch 5/8\n",
      " - 17s - loss: 1.0168e-07\n",
      "Epoch 6/8\n",
      " - 17s - loss: 9.7980e-08\n",
      "Epoch 7/8\n",
      " - 17s - loss: 9.2593e-08\n",
      "Epoch 8/8\n",
      " - 16s - loss: 1.0190e-07\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "198720/198720 [==============================] - 20s 100us/step\n",
      "Training/predicting for level 5\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0063\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0062\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0058\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0040\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 1/8\n",
      " - 18s - loss: 1.0002e-07\n",
      "Epoch 2/8\n",
      " - 18s - loss: 9.6006e-08\n",
      "Epoch 3/8\n",
      " - 17s - loss: 1.0733e-07\n",
      "Epoch 4/8\n",
      " - 18s - loss: 9.6769e-08\n",
      "Epoch 5/8\n",
      " - 17s - loss: 9.5358e-08\n",
      "Epoch 6/8\n",
      " - 16s - loss: 9.5357e-08\n",
      "Epoch 7/8\n",
      " - 16s - loss: 9.4926e-08\n",
      "Epoch 8/8\n",
      " - 18s - loss: 1.0199e-07\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "198720/198720 [==============================] - 18s 91us/step\n",
      "Training/predicting for level 6\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.3260e-07\n",
      "Epoch 2/8\n",
      " - 16s - loss: 1.3682e-07\n",
      "Epoch 3/8\n",
      " - 18s - loss: 1.3923e-07\n",
      "Epoch 4/8\n",
      " - 17s - loss: 1.3217e-07\n",
      "Epoch 5/8\n",
      " - 17s - loss: 1.3688e-07\n",
      "Epoch 6/8\n",
      " - 17s - loss: 1.3405e-07\n",
      "Epoch 7/8\n",
      " - 17s - loss: 1.3771e-07\n",
      "Epoch 8/8\n",
      " - 16s - loss: 1.3457e-07\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "198720/198720 [==============================] - 20s 98us/step\n",
      "Training/predicting for level 7\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0053\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0033\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0041\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0029\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 1/8\n",
      " - 16s - loss: 3.5907e-07\n",
      "Epoch 2/8\n",
      " - 17s - loss: 3.1449e-07\n",
      "Epoch 3/8\n",
      " - 17s - loss: 3.1832e-07\n",
      "Epoch 4/8\n",
      " - 17s - loss: 3.1975e-07\n",
      "Epoch 5/8\n",
      " - 18s - loss: 3.0741e-07\n",
      "Epoch 6/8\n",
      " - 18s - loss: 3.1677e-07\n",
      "Epoch 7/8\n",
      " - 17s - loss: 3.1665e-07\n",
      "Epoch 8/8\n",
      " - 17s - loss: 3.0875e-07\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "198720/198720 [==============================] - 19s 97us/step\n",
      "Training/predicting for level 8\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0120\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0038\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0071\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0050\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0032\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0042\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.2996e-05\n",
      "Epoch 2/8\n",
      " - 17s - loss: 7.6971e-06\n",
      "Epoch 3/8\n",
      " - 18s - loss: 7.7120e-06\n",
      "Epoch 4/8\n",
      " - 17s - loss: 7.7600e-06\n",
      "Epoch 5/8\n",
      " - 18s - loss: 7.6699e-06\n",
      "Epoch 6/8\n",
      " - 17s - loss: 7.7474e-06\n",
      "Epoch 7/8\n",
      " - 17s - loss: 7.7074e-06\n",
      "Epoch 8/8\n",
      " - 16s - loss: 7.6871e-06\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "198720/198720 [==============================] - 20s 103us/step\n",
      "Training/predicting for level 9\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0631\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0344\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0088\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0017\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0076\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0140\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0170\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0178\n",
      "Epoch 1/8\n",
      " - 16s - loss: 1.1979e-04\n",
      "Epoch 2/8\n",
      " - 18s - loss: 6.1101e-05\n",
      "Epoch 3/8\n",
      " - 16s - loss: 6.0301e-05\n",
      "Epoch 4/8\n",
      " - 17s - loss: 6.0129e-05\n",
      "Epoch 5/8\n",
      " - 18s - loss: 5.9988e-05\n",
      "Epoch 6/8\n",
      " - 18s - loss: 5.9882e-05\n",
      "Epoch 7/8\n",
      " - 16s - loss: 5.9683e-05\n",
      "Epoch 8/8\n",
      " - 16s - loss: 5.9582e-05\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "198720/198720 [==============================] - 20s 102us/step\n",
      "Training/predicting for level 10\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0012\n",
      "Epoch 1/8\n",
      " - 18s - loss: 1.5905e-04\n",
      "Epoch 2/8\n",
      " - 17s - loss: 1.3190e-04\n",
      "Epoch 3/8\n",
      " - 18s - loss: 1.3182e-04\n",
      "Epoch 4/8\n",
      " - 17s - loss: 1.3208e-04\n",
      "Epoch 5/8\n",
      " - 17s - loss: 1.3200e-04\n",
      "Epoch 6/8\n",
      " - 17s - loss: 1.3183e-04\n",
      "Epoch 7/8\n",
      " - 16s - loss: 1.3183e-04\n",
      "Epoch 8/8\n",
      " - 16s - loss: 1.3190e-04\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "198720/198720 [==============================] - 20s 99us/step\n",
      "Training/predicting for level 11\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0049\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0047\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0043\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0039\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0030\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 1/8\n",
      " - 17s - loss: 1.8513e-04\n",
      "Epoch 2/8\n",
      " - 17s - loss: 1.8384e-04\n",
      "Epoch 3/8\n",
      " - 17s - loss: 1.8369e-04\n",
      "Epoch 4/8\n",
      " - 18s - loss: 1.8374e-04\n",
      "Epoch 5/8\n",
      " - 17s - loss: 1.8384e-04\n",
      "Epoch 6/8\n",
      " - 18s - loss: 1.8364e-04\n",
      "Epoch 7/8\n",
      " - 17s - loss: 1.8344e-04\n",
      "Epoch 8/8\n",
      " - 17s - loss: 1.8298e-04\n",
      "1/1 [==============================] - 0s 58ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198720/198720 [==============================] - 20s 99us/step\n",
      "Training/predicting for level 12\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0035\n",
      "Epoch 2/8\n",
      " - 0s - loss: 0.0031\n",
      "Epoch 3/8\n",
      " - 0s - loss: 0.0026\n",
      "Epoch 4/8\n",
      " - 0s - loss: 0.0022\n",
      "Epoch 5/8\n",
      " - 0s - loss: 0.0019\n",
      "Epoch 6/8\n",
      " - 0s - loss: 0.0016\n",
      "Epoch 7/8\n",
      " - 0s - loss: 0.0013\n",
      "Epoch 8/8\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 1/8\n",
      " - 17s - loss: 2.2537e-04\n",
      "Epoch 2/8\n",
      " - 17s - loss: 2.2365e-04\n",
      "Epoch 3/8\n",
      " - 17s - loss: 2.2284e-04\n",
      "Epoch 4/8\n",
      " - 18s - loss: 2.2234e-04\n",
      "Epoch 5/8\n",
      " - 18s - loss: 2.2176e-04\n",
      "Epoch 6/8\n",
      " - 18s - loss: 2.2144e-04\n",
      "Epoch 7/8\n",
      " - 17s - loss: 2.2081e-04\n",
      "Epoch 8/8\n",
      " - 18s - loss: 2.2067e-04\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "198720/198720 [==============================] - 19s 97us/step\n",
      "Training/predicting for level 13\n",
      "Epoch 1/8\n",
      " - 0s - loss: 0.0010\n",
      "Epoch 2/8\n",
      " - 0s - loss: 8.5609e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 7.0706e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 5.8965e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.0060e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 4.3634e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 3.9324e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 3.6768e-04\n",
      "Epoch 1/8\n",
      " - 16s - loss: 2.6578e-04\n",
      "Epoch 2/8\n",
      " - 16s - loss: 2.6197e-04\n",
      "Epoch 3/8\n",
      " - 17s - loss: 2.5588e-04\n",
      "Epoch 4/8\n",
      " - 16s - loss: 2.4751e-04\n",
      "Epoch 5/8\n",
      " - 18s - loss: 2.4616e-04\n",
      "Epoch 6/8\n",
      " - 17s - loss: 2.4181e-04\n",
      "Epoch 7/8\n",
      " - 16s - loss: 2.3573e-04\n",
      "Epoch 8/8\n",
      " - 16s - loss: 2.3036e-04\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "198720/198720 [==============================] - 18s 88us/step\n",
      "Training/predicting for level 14\n",
      "Epoch 1/8\n",
      " - 0s - loss: 5.3146e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 5.0209e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 4.8186e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 4.7016e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 4.6556e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 4.6627e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 4.7043e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 4.7618e-04\n",
      "Epoch 1/8\n",
      " - 18s - loss: 2.3884e-04\n",
      "Epoch 2/8\n",
      " - 17s - loss: 2.2872e-04\n",
      "Epoch 3/8\n",
      " - 18s - loss: 2.2856e-04\n",
      "Epoch 4/8\n",
      " - 17s - loss: 2.2437e-04\n",
      "Epoch 5/8\n",
      " - 18s - loss: 2.2150e-04\n",
      "Epoch 6/8\n",
      " - 16s - loss: 2.1438e-04\n",
      "Epoch 7/8\n",
      " - 17s - loss: 2.1226e-04\n",
      "Epoch 8/8\n",
      " - 17s - loss: 2.0601e-04\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "198720/198720 [==============================] - 19s 94us/step\n",
      "Training/predicting for level 15\n",
      "Epoch 1/8\n",
      " - 0s - loss: 5.4998e-04\n",
      "Epoch 2/8\n",
      " - 0s - loss: 5.4944e-04\n",
      "Epoch 3/8\n",
      " - 0s - loss: 5.4860e-04\n",
      "Epoch 4/8\n",
      " - 0s - loss: 5.4700e-04\n",
      "Epoch 5/8\n",
      " - 0s - loss: 5.4440e-04\n",
      "Epoch 6/8\n",
      " - 0s - loss: 5.4074e-04\n",
      "Epoch 7/8\n",
      " - 0s - loss: 5.3607e-04\n",
      "Epoch 8/8\n",
      " - 0s - loss: 5.3055e-04\n",
      "Epoch 1/8\n"
     ]
    }
   ],
   "source": [
    "# loop over all levels\n",
    "for lev in range(size_lev):\n",
    "    \n",
    "    print(\"Training/predicting for level {level}\".format(level=lev))\n",
    "    \n",
    "    # get the features and labels for training\n",
    "    train_x, train_y = extract_features_labels(netcdf_features_train[0],\n",
    "                                               netcdf_labels_train[0],\n",
    "                                               features,\n",
    "                                               labels,\n",
    "                                               lev)\n",
    "    \n",
    "    # get the features for prediction\n",
    "    predict_x = extract_data_array(xr.open_dataset(netcdf_features_predict[0]),\n",
    "                                   features,\n",
    "                                   lev)\n",
    "\n",
    "    # scale the data between 0 and 1\n",
    "    scalers_x = [MinMaxScaler(feature_range=(0, 1))] * len(features)\n",
    "    scalers_y = [MinMaxScaler(feature_range=(0, 1))] * len(labels)\n",
    "    scaled_train_x, scaled_predict_x, scaled_train_y = scale_4d(train_x, \n",
    "                                                                predict_x, \n",
    "                                                                train_y, \n",
    "                                                                scalers_x, \n",
    "                                                                scalers_y)\n",
    "    \n",
    "    # reshape the data for convolutional model input\n",
    "    shape_x = (1, ) + scaled_train_x.shape\n",
    "    shape_y = (1, ) + scaled_train_y.shape\n",
    "    train_x_cnn = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "    train_y_cnn = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "    predict_x_cnn = np.reshape(scaled_predict_x, newshape=shape_x)\n",
    "    \n",
    "    # reshape the data for dense layer model input\n",
    "    shape_x = (size_times_train * size_lat * size_lon, len(features))\n",
    "    shape_y = (size_times_train * size_lat * size_lon, len(labels))\n",
    "    train_x_dense = np.reshape(scaled_train_x, newshape=shape_x)\n",
    "    train_y_dense = np.reshape(scaled_train_y, newshape=shape_y)\n",
    "    predict_x_dense = np.reshape(scaled_predict_x, newshape=shape_x)\n",
    "    \n",
    "    # train the models\n",
    "    cnn_model.fit(train_x_cnn, train_y_cnn, shuffle=True, epochs=8, verbose=2)\n",
    "    dense_model.fit(train_x_dense, train_y_dense, shuffle=True, epochs=8, verbose=2)\n",
    "    \n",
    "    # use the fitted models to make predictions\n",
    "    predict_y_scaled_cnn = cnn_model.predict(predict_x_cnn, verbose=1)\n",
    "    predict_y_scaled_dense = dense_model.predict(predict_x_dense, verbose=1)\n",
    "\n",
    "    # reverse the scaling of the predicted values\n",
    "    # TODO below assumes a single label, will need modification for multiple labels\n",
    "    scaler = scalers_y[0]  # assumes the label scaler was fitted in scale_4d() and side effect carried through\n",
    "        \n",
    "    # output from the dense model is 2-D, good for scaler input\n",
    "    unscaled_predict_y_dense = scaler.inverse_transform(predict_y_scaled_dense)\n",
    "        \n",
    "    # output from CNN model is 5-D, so we'll flatten first to make it amenable to scaling\n",
    "    unscaled_predict_y_cnn = scaler.inverse_transform(predict_y_scaled_cnn.flatten().reshape(-1, 1))\n",
    "    \n",
    "    # reshape data so it's compatible with assignment into prediction arrays\n",
    "    level_shape = (size_times_predict, size_lat, size_lon)\n",
    "    prediction_cnn[:, lev, :, :] = np.reshape(unscaled_predict_y_cnn, newshape=level_shape)\n",
    "    prediction_dense[:, lev, :, :] = np.reshape(unscaled_predict_y_dense, newshape=level_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output of predicted values as NetCDF\n",
    "At this point the entire dataset has been predicted and the predicted values are in arrays that we can add to an xarray DataSet that we'll then write as NetCDF. We will copy the prediction features dataset sicne this has all the coordinate variables and attributes we'll want in the predicted labels dataset, then we'll add the predicted values arrays as variables to the dataset before writing it as NetCDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the prediction features dataset since the predicted label(s) should share the same coordinates, etc.\n",
    "ds_predict_labels = ds_predict_features.copy(deep=True)\n",
    "\n",
    "# remove all non-label data variables from the predictions dataset\n",
    "for var in ds_predict_labels.data_vars:\n",
    "    if var not in labels:\n",
    "        ds_predict_labels = ds_predict_labels.drop(var)\n",
    "\n",
    "# create new variables to contain the predicted labels, assign these into the prediction dataset\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=prediction_cnn,\n",
    "                                  attrs=ds_learn_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_cnn\"] = predicted_label_var\n",
    "predicted_label_var = xr.Variable(dims=('time', 'lev', 'lat', 'lon'),\n",
    "                                  data=prediction_dense,\n",
    "                                  attrs=ds_learn_labels[labels[0]].attrs)\n",
    "ds_predict_labels[labels[0] + \"_dense\"] = predicted_label_var\n",
    "\n",
    "# write the predicted label(s)' dataset as a NetCDF file\n",
    "ds_predict_labels.to_netcdf(netcdf_predict[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
